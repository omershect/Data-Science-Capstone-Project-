---
title: "Capstone Project - Mile Stone Report"
author: "Omer Shechter"
date: "6 January 2019"
---

## Executive summary.  
This is the milestone report, for the Data science capstone project.
This report contains the following items:
1. A module that downloads the data.
2. A  fundamental exploratory analysis of the data. 
3. A module with more in-depth exploratory study and a portion (sample of the data set) to get some sense of how the data set looks like. 
4. Next steps description


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#Load libraries 
library(kableExtra)
library(dplyr)
library(tidytext)
library(ggplot2)
library(quanteda)
#library(tm)
```
## Load the Data 

### Read the data and unzip it
```{r echo = TRUE}
#Link to the Location of the Text data file 
UrlLink<-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
#Create Data Directory (If not exist)
if(!file.exists("data")) {  
        dir.create("data")  
} 
#Download and Unzip the file (if not exist)
if(!file.exists("./data/Coursera-SwiftKey.zip")) { 
    download.file(UrlLink, destfile="./data/Coursera-SwiftKey.zip")  
        if(!file.exists("./data/final"))
          unzip("./data/Coursera-SwiftKey.zip",exdir="./data") 
}
```
### Read the texts from the files 
```{r echo=TRUE, warning = FALSE}
en_US.Blogs <- readLines(con = "./data/final/en_US/en_US.blogs.txt")
en_US.news <- readLines(con = "./data/final/en_US/en_US.news.txt")
en_US.twitter <- readLines(con = "./data/final/en_US/en_US.twitter.txt")
```
## Basic Statistics
Build a table of basic statistics of the three Datasets: 
1. Number of Lines. 
2. The number of sentences. 
3. The number of characters. 

### Lines(sentences), word, and characters statistics
```{r echo=FALSE,message=FALSE}
#Calculate the number of lines 
USBlogsLines<-length(en_US.Blogs)
USBnewsLines<-length(en_US.news)
USBtwitterLines<-length(en_US.twitter)

#Calculate the number of words in each sentence
USBlogsWords<-sapply(strsplit(en_US.Blogs, " "), length)
USNewsWords<-sapply(strsplit(en_US.news, " "), length)
USTwitterWords<-sapply(strsplit(en_US.twitter, " "), length)

#Calculate the total number of words
TotalUSBlogsWords<-sum(USBlogsWords)
TotalUSNewsWords<-sum(USNewsWords)
TotalUSTwitterWords<-sum(USTwitterWords)

# Calculate the number of chars in each sentence 
USBlogsChars<-sapply(en_US.Blogs, nchar)
USNewsChars<-sapply(en_US.news, nchar)
USTwitterChars<-sapply(en_US.twitter, nchar)

#Total the number of characters
TotalUSBlogschars<-sum(USBlogsChars)
TotalUSNewschars<-sum(USNewsChars)
TotalUSTwitterchars<-sum(USTwitterChars)

DF<-data.frame(USBlogsLines,USBnewsLines,USBtwitterLines)
names(DF)<-c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt")
rownames(DF)<-"Lines"

#Pack in one data frame to create one table
TotalWords<-data.frame(TotalUSBlogsWords,TotalUSNewsWords,TotalUSTwitterWords)
names(TotalWords)<-c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt")
rownames(TotalWords)<-"Total Number of Words"
Totalchars<-data.frame(TotalUSBlogschars,TotalUSNewschars,TotalUSTwitterchars)
names(Totalchars)<-c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt")
rownames(Totalchars)<-"Total Number of Chars"

DF<-rbind(DF,TotalWords,Totalchars)

DF%>%
  kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

### Sample - For the Exploratory purpose 
Sample - for the exploratory purpose, 
The amount  of data is significant, 
doing the calculation on the entire 
a population of that will take a long time 
and consume a high volume of memory 
```{r echo=TRUE,message=TRUE}
set.seed(534)
Sample.Text<-c(sample(en_US.Blogs,length(en_US.Blogs) * (10/100),replace = FALSE),
               sample(en_US.news,length(en_US.news) * (10/100),replace = FALSE),
               sample(en_US.twitter,length(en_US.twitter) * (10/100),replace = FALSE))
               
 rm(en_US.Blogs)
 rm(en_US.news)
 rm(en_US.twitter) 
```

### Data Cleaning
The cleaning process includes:
Tokenize into sentences (each line may contain more than one sentence)
Remove sentence which has 
non-English phrases 
Remove Punctuation
Remove @hashtags
Remove URLs
Remove Stop Words (For the Exploratory step Stopwords are removed)
```{r echo=TRUE,message=TRUE}
#Convert the Sample.txt vector to a Data frame 
DF <- data_frame(text = Sample.Text)
rm(Sample.Text)

#Split of sentences (Each line may contain more than one sentence)
Text.Sentences<-DF %>%
  unnest_tokens(sentence, text, token = "sentences")


#Convert all non-Engish words to NA and remove them
Text.Sentences<-lapply( Text.Sentences$sentence, iconv,from = "latin1", to = "ASCII")
#If a sentence contains non-English word or character remove the entire sentence 
Text.Sentences <- Text.Sentences[!is.na(Text.Sentences)]

#Remove punctuation, Twitter, Numbers, Hyphens, symbols, and URL  
Text.Sentences <- tokens(
    x = tolower(Text.Sentences),
    remove_punct = TRUE,
    remove_twitter = TRUE,
    remove_numbers = TRUE,
    remove_hyphens = TRUE,
    remove_symbols = TRUE,
    remove_url = TRUE)
#Remove Stop Words 
Text.Sentences <- tokens_remove(Text.Sentences,stopwords("english"))

```

# Create 1 gram word's Statistics
Create 1 gram word's statistics and plot it using ggplot2. 
```{r}
DF1grham <- dfm(Text.Sentences, verbose = TRUE, 
               stem = TRUE)
top30Onegrams <- topfeatures(DF1grham, 30)  # 30 top words
topwordsDF<-data.frame(names(top30Onegrams),top30Onegrams)
names(topwordsDF)<-c("Words","Frequancy")
ggplot(data=topwordsDF,aes(reorder(Words,-Frequancy),Frequancy))+geom_bar(stat = "identity",color = "black", fill = I("red"))+
  theme(axis.title.x = element_text(face="bold", colour="#990000", size=20),axis.title.y = element_text(face="bold", colour="#990000", size=20), axis.text.x  = element_text(angle=45, vjust=0.5, size=12))+xlab("Word") 

```

# Create 2 grams word's Statistics
Create 2 grams word's statistics and plot it using ggplot2. 
```{r}
DF2grham <- dfm(Text.Sentences, ngrams=2,verbose = TRUE, 
               stem = FALSE)
top30Onegrams <- topfeatures(DF2grham, 30)  # 30 top words
topwordsDF<-data.frame(names(top30Onegrams),top30Onegrams)
names(topwordsDF)<-c("Words","Frequancy")
ggplot(data=topwordsDF,aes(reorder(Words,-Frequancy),Frequancy))+geom_bar(stat = "identity", color = "black",fill = I("green"))+
  theme(axis.title.x = element_text(face="bold", colour="#990000", size=20),axis.title.y = element_text(face="bold", colour="#990000", size=20), axis.text.x  = element_text(angle=45, vjust=0.5, size=12))+xlab("Words pair") 

```

#Create 3 grams word's Statisticss
Create 3 grams word's statistics and plot it using ggplot2. 
```{r}
DF3grham <- dfm(Text.Sentences, ngrams=3,verbose = TRUE,
               stem = FALSE)
top30Onegrams <- topfeatures(DF3grham, 30)  # 30 top words
topwordsDF<-data.frame(names(top30Onegrams),top30Onegrams)
names(topwordsDF)<-c("Words","Frequancy")
ggplot(data=topwordsDF,aes(reorder(Words,-Frequancy),Frequancy))+geom_bar(stat = "identity", color = "black",fill = I("blue"))+
  theme(axis.title.x = element_text(face="bold", colour="#990000", size=20),axis.title.y = element_text(face="bold", colour="#990000", size=20), axis.text.x  = element_text(angle=45, vjust=0.5, size=12))+xlab("Words pair") 

```

# Observations 
1. I used the Quanteda library - it looks like it gives good and efficient performances.


#Next steps ...
1. Build a full 1:3 grams model and dictionary to be used for the prediction algorithm×¥ 
2. Select a smoothing method. Although this is a significant word sample, it will not be enough, and words are missing (overcome the issue of giving probability zero to unseen words)
3. Program the model 
4. Select an efficient way how to save the data (n-grams) in memory or file
5. Build the Shiny server and UI 
